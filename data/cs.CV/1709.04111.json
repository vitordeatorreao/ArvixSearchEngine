{
    "paper_authors_list": [
        "Shen, Falong", 
        "Yan, Shuicheng", 
        "Zeng, Gang"
    ], 
    "paper_comments": null, 
    "paper_page_url": "https://arxiv.org/abs/1709.04111", 
    "paper_abstract": "In this paper we propose a new method to get the specified network parameters\nthrough one time feed-forward propagation of the meta networks and explore the\napplication to neural style transfer. Recent works on style transfer typically\nneed to train image transformation networks for every new style, and the style\nis encoded in the network parameters by enormous iterations of stochastic\ngradient descent. To tackle these issues, we build a meta network which takes\nin the style image and produces a corresponding image transformations network\ndirectly. Compared with optimization-based methods for every style, our meta\nnetworks can handle an arbitrary new style within $19ms$ seconds on one modern\nGPU card. The fast image transformation network generated by our meta network\nis only 449KB, which is capable of real-time executing on a mobile device. We\nalso investigate the manifold of the style transfer networks by operating the\nhidden features from meta networks. Experiments have well validated the\neffectiveness of our method. Code and trained models has been released\n<a href=\"https://github.com/FalongShen/styletransfer.\">this https URL</a>", 
    "paper_subjects": null, 
    "paper_code": "1709.04111", 
    "paper_submission_date": "2017/09/13", 
    "paper_title": "Meta Networks for Neural Style Transfer"
}