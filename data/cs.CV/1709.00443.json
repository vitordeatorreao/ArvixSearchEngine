{
    "paper_authors_list": [
        "Petridis, Stavros", 
        "Wang, Yujiang", 
        "Li, Zuwei", 
        "Pantic, Maja"
    ], 
    "paper_comments": "Accepted to BMVC 2017", 
    "paper_page_url": "https://arxiv.org/abs/1709.00443", 
    "paper_abstract": "Non-frontal lip views contain useful information which can be used to enhance\nthe performance of frontal view lipreading. However, the vast majority of\nrecent lipreading works, including the deep learning approaches which\nsignificantly outperform traditional approaches, have focused on frontal mouth\nimages. As a consequence, research on joint learning of visual features and\nspeech classification from multiple views is limited. In this work, we present\nan end-to-end multi-view lipreading system based on Bidirectional Long-Short\nMemory (BLSTM) networks. To the best of our knowledge, this is the first model\nwhich simultaneously learns to extract features directly from the pixels and\nperforms visual speech classification from multiple views and also achieves\nstate-of-the-art performance. The model consists of multiple identical streams,\none for each view, which extract features directly from different poses of\nmouth images. The temporal dynamics in each stream/view are modelled by a BLSTM\nand the fusion of multiple streams/views takes place via another BLSTM. An\nabsolute average improvement of 3% and 3.8% over the frontal view performance\nis reported on the OuluVS2 database when the best two (frontal and profile) and\nthree views (frontal, profile, 45) are combined, respectively. The best\nthree-view model results in a 10.5% absolute improvement over the current\nmulti-view state-of-the-art performance on OuluVS2, without using external\ndatabases for training, achieving a maximum classification accuracy of 96.9%.", 
    "paper_subjects": null, 
    "paper_code": "1709.00443", 
    "paper_submission_date": "2017/09/01", 
    "paper_title": "End-to-End Multi-View Lipreading"
}