{
    "paper_authors_list": [
        "Hou, Jen-Cheng", 
        "Wang, Syu-Siang", 
        "Lai, Ying-Hui", 
        "Tsao, Yu", 
        "Chang, Hsiu-Wen", 
        "Wang, Hsin-Min"
    ], 
    "paper_comments": "", 
    "paper_page_url": "https://arxiv.org/abs/1709.00944", 
    "paper_abstract": "Speech enhancement (SE) aims to reduce noise in speech signals. Most SE\ntechniques focus on addressing audio information only. In this work, inspired\nby multimodal learning, which utilizes data from different modalities, and the\nrecent success of convolutional neural networks (CNNs) in SE, we propose an\naudio-visual deep CNN (AVDCNN) SE model, which incorporates audio and visual\nstreams into a unified network model. In the proposed AVDCNN SE model, audio\nand visual data are first processed using individual CNNs, and then, fused into\na joint network to generate enhanced speech at the output layer. The AVDCNN\nmodel is trained in an end-to-end manner, and parameters are jointly learned\nthrough back-propagation. We evaluate enhanced speech using five objective\ncriteria. Results show that the AVDCNN yields notably better performance,\ncompared with an audio-only CNN-based SE model and two conventional SE\napproaches, confirming the effectiveness of integrating visual information into\nthe SE process.", 
    "paper_subjects": [
        "Multimedia (cs.MM)", 
        "Machine Learning (stat.ML)"
    ], 
    "paper_code": "1709.00944", 
    "paper_submission_date": "2017/09/01", 
    "paper_title": "Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional Neural Network"
}