{
    "paper_authors_list": [
        "Tayyub, Jawad", 
        "Hawasly, Majd", 
        "Hogg, David C.", 
        "Cohn, Anthony G."
    ], 
    "paper_comments": null, 
    "paper_page_url": "https://arxiv.org/abs/1709.03456", 
    "paper_abstract": "This paper introduces a novel activity dataset which exhibits real-life and\ndiverse scenarios of complex, temporally-extended human activities and actions.\nThe dataset presents a set of videos of actors performing everyday activities\nin a natural and unscripted manner. The dataset was recorded using a static\nKinect 2 sensor which is commonly used on many robotic platforms. The dataset\ncomprises of RGB-D images, point cloud data, automatically generated skeleton\ntracks in addition to crowdsourced annotations. Furthermore, we also describe\nthe methodology used to acquire annotations through crowdsourcing. Finally some\nactivity recognition benchmarks are presented using current state-of-the-art\ntechniques. We believe that this dataset is particularly suitable as a testbed\nfor activity recognition research but it can also be applicable for other\ncommon tasks in robotics/computer vision research such as object detection and\nhuman skeleton tracking.", 
    "paper_subjects": [
        "Artificial Intelligence (cs.AI)"
    ], 
    "paper_code": "1709.03456", 
    "paper_submission_date": "2017/09/11", 
    "paper_title": "CLAD: A Complex and Long Activities Dataset with Rich Crowdsourced Annotations"
}