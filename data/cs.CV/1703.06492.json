{
    "paper_authors_list": [
        "Huang, Jia-Hong", 
        "Alfadly, Modar", 
        "Ghanem, Bernard"
    ], 
    "paper_comments": "Accepted by CVPR 2017 VQA Challenge Workshop. (Tables updated)", 
    "paper_page_url": "https://arxiv.org/abs/1703.06492", 
    "paper_abstract": "Taking an image and question as the input of our method, it can output the\ntext-based answer of the query question about the given image, so called Visual\nQuestion Answering (VQA). There are two main modules in our algorithm. Given a\nnatural language question about an image, the first module takes the question\nas input and then outputs the basic questions of the main given question. The\nsecond module takes the main question, image and these basic questions as input\nand then outputs the text-based answer of the main question. We formulate the\nbasic questions generation problem as a LASSO optimization problem, and also\npropose a criterion about how to exploit these basic questions to help answer\nmain question. Our method is evaluated on the challenging VQA dataset and\nyields state-of-the-art accuracy, 60.34% in open-ended task.", 
    "paper_subjects": [
        "Computation and Language (cs.CL)"
    ], 
    "paper_code": "1703.06492", 
    "paper_submission_date": "2017/03/19", 
    "paper_title": "VQABQ: Visual Question Answering by Basic Questions"
}