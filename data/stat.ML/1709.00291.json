{
    "paper_authors_list": [
        "Tadic, Vladislav B.", 
        "Doucet, Arnaud"
    ], 
    "paper_comments": "", 
    "paper_page_url": "https://arxiv.org/abs/1709.00291", 
    "paper_abstract": "The asymptotic behavior of the stochastic gradient algorithm with a biased\ngradient estimator is analyzed. Relying on arguments based on the dynamic\nsystem theory (chain-recurrence) and the differential geometry (Yomdin theorem\nand Lojasiewicz inequality), tight bounds on the asymptotic bias of the\niterates generated by such an algorithm are derived. The obtained results hold\nunder mild conditions and cover a broad class of high-dimensional nonlinear\nalgorithms. Using these results, the asymptotic properties of the\npolicy-gradient (reinforcement) learning and adaptive population Monte Carlo\nsampling are studied. Relying on the same results, the asymptotic behavior of\nthe recursive maximum split-likelihood estimation in hidden Markov models is\nanalyzed, too.", 
    "paper_subjects": [
        "Optimization and Control (math.OC)", 
        "Machine Learning (stat.ML)"
    ], 
    "paper_code": "1709.00291", 
    "paper_submission_date": "2017/08/30", 
    "paper_title": "Asymptotic Bias of Stochastic Gradient Search"
}